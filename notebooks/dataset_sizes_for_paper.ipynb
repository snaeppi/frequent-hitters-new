{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset size summaries for frequent-hitters\n",
    "\n",
    "This notebook computes dataset sizes and filtering statistics used in the paper, based on the PubChem HTS data and the downstream cleaning pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, Dict\n",
    "\n",
    "import polars as pl\n",
    "from rdkit import Chem, RDLogger\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.Chem.MolStandardize import rdMolStandardize\n",
    "\n",
    "# Silence RDKit warnings\n",
    "RDLogger.logger().setLevel(RDLogger.CRITICAL)\n",
    "\n",
    "# Show Polars version for reproducibility\n",
    "print(\"Polars version:\", pl.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust these paths if you run the notebook from a different working directory.\n",
    "project_root = Path.cwd().parent\n",
    "\n",
    "assay_tables_dir = project_root / \"pubchem-bioassay\" / \"data\" / \"assay_tables\"\n",
    "assay_metadata_path = project_root / \"pubchem-bioassay\" / \"outputs\" / \"assay_metadata.csv\"\n",
    "assay_rscores_path = project_root / \"pubchem-bioassay\" / \"outputs\" / \"assay_rscores.parquet\"\n",
    "\n",
    "# Paths to clean-split outputs for the full dataset (produced on the cluster).\n",
    "# Update these to point at the actual cleaned files.\n",
    "clean_biochemical_path = project_root / \"cleaned\" / \"biochemical_hits.parquet\"\n",
    "clean_cellular_path = project_root / \"cleaned\" / \"cellular_hits.parquet\"\n",
    "\n",
    "# For local testing you can instead point to the integration subset:\n",
    "# integration_raw = project_root / \"integration_artifacts\" / \"raw\"\n",
    "# clean_biochemical_path = integration_raw / \"biochemical_hits_subset.parquet\"\n",
    "# clean_cellular_path = integration_raw / \"cellular_hits_subset.parquet\"\n",
    "\n",
    "print(\"Assay tables dir:\", assay_tables_dir)\n",
    "print(\"Assay metadata:\", assay_metadata_path)\n",
    "print(\"Assay rscores:\", assay_rscores_path)\n",
    "print(\"Clean biochemical path:\", clean_biochemical_path)\n",
    "print(\"Clean cellular path:\", clean_cellular_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Assays with \u226510k substances and unique compounds\n",
    "\n",
    "Counts based on the pre-filtered assay tables in `pubchem-bioassay/data/assay_tables`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List assay table parquet files (each file corresponds to one assay with \u226510k substances).\n",
    "assay_table_files = sorted(assay_tables_dir.glob(\"aid_*.parquet\"))\n",
    "\n",
    "assay_table_df = pl.DataFrame(\n",
    "    {\n",
    "        \"assay_id\": [int(p.stem.split(\"_\")[1]) for p in assay_table_files],\n",
    "        \"path\": [str(p) for p in assay_table_files],\n",
    "    }\n",
    ")\n",
    "\n",
    "num_assays_ge_10k = assay_table_df.height\n",
    "print(f\"Number of assays with \u226510k substances (assay tables): {num_assays_ge_10k}\")\n",
    "\n",
    "# Compute total numbers of substances (SIDs) and compounds (CIDs) across all assays.\n",
    "assay_tables_lf = pl.scan_parquet(str(assay_tables_dir / \"aid_*.parquet\"))\n",
    "\n",
    "assay_level_stats = assay_tables_lf.select(\n",
    "    pl.len().alias(\"num_rows\"),\n",
    "    pl.col(\"PUBCHEM_SID\").n_unique().alias(\"num_unique_substances\"),\n",
    "    pl.col(\"PUBCHEM_CID\").n_unique().alias(\"num_unique_compounds\"),\n",
    ").collect()\n",
    "\n",
    "assay_level_stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Assay and compound counts after column selection\n",
    "\n",
    "Uses `assay_metadata.csv` to quantify how many assays/compounds are retained vs ineligible after selecting a single readout column.\n",
    "\n",
    "`compounds_screened` counts the number of compound\u2013assay screening pairs (not unique compounds).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_meta = pl.read_csv(assay_metadata_path, infer_schema_length=1000)\n",
    "\n",
    "total_assays = assay_meta.height\n",
    "num_ineligible_assays = assay_meta.filter(pl.col(\"selected_column\") == \"__INELIGIBLE__\").height\n",
    "num_eligible_assays = total_assays - num_ineligible_assays\n",
    "\n",
    "assay_selection_summary = pl.DataFrame(\n",
    "    {\n",
    "        \"total_assays\": [total_assays],\n",
    "        \"eligible_assays\": [num_eligible_assays],\n",
    "        \"ineligible_assays\": [num_ineligible_assays],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Assay counts after column selection:\")\n",
    "assay_selection_summary\n",
    "\n",
    "compound_selection_summary = assay_meta.select(\n",
    "    pl.len().alias(\"num_assays\"),\n",
    "    pl.col(\"compounds_screened\").sum().alias(\"total_compounds_screened\"),\n",
    "    pl.when(pl.col(\"selected_column\") != \"__INELIGIBLE__\")\n",
    "    .then(pl.col(\"compounds_screened\"))\n",
    "    .otherwise(0)\n",
    "    .sum()\n",
    "    .alias(\"compounds_screened_eligible\"),\n",
    "    pl.when(pl.col(\"selected_column\") == \"__INELIGIBLE__\")\n",
    "    .then(pl.col(\"compounds_screened\"))\n",
    "    .otherwise(0)\n",
    "    .sum()\n",
    "    .alias(\"compounds_screened_ineligible\"),\n",
    ")\n",
    "\n",
    "print(\"Compound (screening) counts based on assay_metadata:\")\n",
    "compound_selection_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Assay format breakdown (biochemical vs cellular vs other)\n",
    "\n",
    "Counts of assays by `assay_format` from `assay_metadata.csv`, before and after applying the column-selection eligibility filter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_format_total = (\n",
    "    assay_meta\n",
    "    .group_by(\"assay_format\")\n",
    "    .agg(pl.len().alias(\"num_assays\"))\n",
    "    .sort(\"assay_format\")\n",
    ")\n",
    "\n",
    "assay_format_eligible = (\n",
    "    assay_meta\n",
    "    .filter(pl.col(\"selected_column\") != \"__INELIGIBLE__\")\n",
    "    .group_by(\"assay_format\")\n",
    "    .agg(pl.len().alias(\"num_assays_eligible\"))\n",
    "    .sort(\"assay_format\")\n",
    ")\n",
    "\n",
    "assay_format_ineligible = (\n",
    "    assay_meta\n",
    "    .filter(pl.col(\"selected_column\") == \"__INELIGIBLE__\")\n",
    "    .group_by(\"assay_format\")\n",
    "    .agg(pl.len().alias(\"num_assays_ineligible\"))\n",
    "    .sort(\"assay_format\")\n",
    ")\n",
    "\n",
    "print(\"Assay counts by format (all assays):\")\n",
    "print(assay_format_total)\n",
    "\n",
    "print(\"\\nAssay counts by format (eligible only):\")\n",
    "print(assay_format_eligible)\n",
    "\n",
    "print(\"\\nAssay counts by format (ineligible only):\")\n",
    "print(assay_format_ineligible)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Overall impact of clean-split on HTS data\n",
    "\n",
    "Summaries before/after cleaning using the HTS hits table (`assay_rscores.parquet`) and the biochemical/cellular outputs produced by the `clean-split` CLI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of the raw HTS hits table used as input to clean-split.\n",
    "hts_lf = pl.scan_parquet(assay_rscores_path)\n",
    "\n",
    "hts_summary = hts_lf.select(\n",
    "    pl.len().alias(\"num_rows\"),\n",
    "    pl.col(\"assay_id\").n_unique().alias(\"num_assays\"),\n",
    "    pl.col(\"compound_id\").n_unique().alias(\"num_unique_compounds\"),\n",
    ").collect()\n",
    "\n",
    "print(\"Raw HTS hits (assay_rscores.parquet):\")\n",
    "hts_summary\n",
    "\n",
    "# Summaries for the cleaned biochemical / cellular outputs (if available).\n",
    "clean_lfs = []\n",
    "if clean_biochemical_path.is_file():\n",
    "    clean_lfs.append(pl.scan_parquet(str(clean_biochemical_path)))\n",
    "if clean_cellular_path.is_file():\n",
    "    clean_lfs.append(pl.scan_parquet(str(clean_cellular_path)))\n",
    "\n",
    "if clean_lfs:\n",
    "    clean_lf = clean_lfs[0]\n",
    "    for lf in clean_lfs[1:]:\n",
    "        clean_lf = clean_lf.union(lf)\n",
    "\n",
    "    clean_summary = clean_lf.select(\n",
    "        pl.len().alias(\"num_rows\"),\n",
    "        pl.col(\"assay_id\").n_unique().alias(\"num_assays\"),\n",
    "        pl.col(\"compound_id\").n_unique().alias(\"num_unique_compounds\"),\n",
    "    ).collect()\n",
    "\n",
    "    print(\"Cleaned HTS hits after clean-split (combined biochemical + cellular):\")\n",
    "    clean_summary\n",
    "\n",
    "    pre_rows = int(hts_summary[\"num_rows\"][0])\n",
    "    pre_compounds = int(hts_summary[\"num_unique_compounds\"][0])\n",
    "    post_rows = int(clean_summary[\"num_rows\"][0])\n",
    "    post_compounds = int(clean_summary[\"num_unique_compounds\"][0])\n",
    "\n",
    "    drop_summary = pl.DataFrame(\n",
    "        {\n",
    "            \"num_rows_before\": [pre_rows],\n",
    "            \"num_rows_after\": [post_rows],\n",
    "            \"num_rows_dropped\": [pre_rows - post_rows],\n",
    "            \"num_unique_compounds_before\": [pre_compounds],\n",
    "            \"num_unique_compounds_after\": [post_compounds],\n",
    "            \"num_unique_compounds_dropped\": [pre_compounds - post_compounds],\n",
    "        }\n",
    "    )\n",
    "    print(\"Overall rows / unique-compound counts before vs after clean-split:\")\n",
    "    drop_summary\n",
    "else:\n",
    "    print(\n",
    "        \"Clean-split outputs not found at the configured paths; \"\n",
    "        \"skipping pre/post clean-split summary.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Breakdown of compounds dropped by clean-split\n",
    "\n",
    "This section mirrors the `ChemicalProcessor` logic from `clean_split.cli` to assign each compound a reason for being dropped (invalid SMILES, molecular weight filter, disallowed elements, etc.).\n",
    "\n",
    "Note: this re-runs RDKit-based standardisation on the unique SMILES and is therefore expensive on the full dataset. Run this on the cluster rather than on a local laptop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants as in clean_split.cli\n",
    "ALLOWED_ATOMS = {\n",
    "    1,   # H\n",
    "    5,   # B\n",
    "    6,   # C\n",
    "    7,   # N\n",
    "    8,   # O\n",
    "    9,   # F\n",
    "    14,  # Si\n",
    "    15,  # P\n",
    "    16,  # S\n",
    "    17,  # Cl\n",
    "    34,  # Se\n",
    "    35,  # Br\n",
    "    53,  # I\n",
    "}\n",
    "\n",
    "MIN_MW = 180.0\n",
    "MAX_MW = 900.0\n",
    "\n",
    "UNCHARGER = rdMolStandardize.Uncharger()\n",
    "FRAGMENT_CHOOSER = rdMolStandardize.LargestFragmentChooser()\n",
    "TAUT_ENUMERATOR = rdMolStandardize.TautomerEnumerator()\n",
    "\n",
    "def clean_smiles_with_reason(smi: Optional[str]) -> Tuple[Optional[str], str]:\n",
    "    \"\"\"Apply the same series of steps as clean_split.ChemicalProcessor,\n",
    "    but return both the canonical SMILES (or None) and a reason label.\"\"\"\n",
    "    if smi is None:\n",
    "        return None, \"missing_smiles\"\n",
    "\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "    except Exception:\n",
    "        return None, \"invalid_smiles_parse_error\"\n",
    "\n",
    "    if mol is None:\n",
    "        return None, \"invalid_smiles_none\"\n",
    "\n",
    "    try:\n",
    "        mol = UNCHARGER.uncharge(mol)\n",
    "        mol = FRAGMENT_CHOOSER.choose(mol)\n",
    "        mol = UNCHARGER.uncharge(mol)\n",
    "        mol = TAUT_ENUMERATOR.Canonicalize(mol)\n",
    "    except Exception:\n",
    "        return None, \"standardisation_error\"\n",
    "\n",
    "    if mol is None:\n",
    "        return None, \"standardisation_error\"\n",
    "\n",
    "    try:\n",
    "        mw = Descriptors.ExactMolWt(mol)\n",
    "    except Exception:\n",
    "        return None, \"mw_compute_error\"\n",
    "\n",
    "    if not (MIN_MW <= mw <= MAX_MW):\n",
    "        return None, \"molecular_weight_filter\"\n",
    "\n",
    "    for atom in mol.GetAtoms():\n",
    "        if atom.GetAtomicNum() not in ALLOWED_ATOMS:\n",
    "            return None, \"disallowed_element\"\n",
    "\n",
    "    try:\n",
    "        canonical = Chem.MolToSmiles(mol, canonical=True, isomericSmiles=False)\n",
    "    except Exception:\n",
    "        return None, \"canonicalisation_error\"\n",
    "\n",
    "    try:\n",
    "        if Chem.MolFromSmiles(canonical) is None:\n",
    "            return None, \"canonicalisation_invalid_smiles\"\n",
    "    except Exception:\n",
    "        return None, \"canonicalisation_invalid_smiles\"\n",
    "\n",
    "    return canonical, \"retained\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the compound_id -> SMILES mapping exactly as in clean-split.\n",
    "id_to_smiles = (\n",
    "    pl.scan_parquet(assay_rscores_path)\n",
    "    .select(\"compound_id\", \"smiles\")\n",
    "    .unique()\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "original_unique_compounds = id_to_smiles.height\n",
    "print(f\"Original unique compounds (before clean-split): {original_unique_compounds:,}\")\n",
    "\n",
    "# Run the RDKit-based cleaning once per unique SMILES string.\n",
    "unique_smiles = id_to_smiles[\"smiles\"].unique().to_list()\n",
    "print(f\"Number of unique SMILES to process: {len(unique_smiles):,}\")\n",
    "\n",
    "canonical_map: Dict[str, Tuple[Optional[str], str]] = {}\n",
    "for smi in unique_smiles:\n",
    "    canonical, reason = clean_smiles_with_reason(smi)\n",
    "    canonical_map[smi] = (canonical, reason)\n",
    "\n",
    "# Attach canonical SMILES and reason to the per-compound table.\n",
    "id_to_smiles_with_reasons = id_to_smiles.with_columns(\n",
    "    pl.col(\"smiles\")\n",
    "    .map_elements(lambda smi: canonical_map[smi][0], return_dtype=pl.Utf8)\n",
    "    .alias(\"canonical_smiles\"),\n",
    "    pl.col(\"smiles\")\n",
    "    .map_elements(lambda smi: canonical_map[smi][1], return_dtype=pl.Utf8)\n",
    "    .alias(\"clean_reason\"),\n",
    ")\n",
    "\n",
    "retained_unique_compounds = id_to_smiles_with_reasons.filter(\n",
    "    pl.col(\"canonical_smiles\").is_not_null()\n",
    ").height\n",
    "dropped_unique_compounds = original_unique_compounds - retained_unique_compounds\n",
    "\n",
    "print(f\"Retained unique compounds after clean-split: {retained_unique_compounds:,}\")\n",
    "print(f\"Dropped unique compounds after clean-split: {dropped_unique_compounds:,}\")\n",
    "\n",
    "# Breakdown of dropped compounds by reason.\n",
    "reason_counts = (\n",
    "    id_to_smiles_with_reasons\n",
    "    .group_by(\"clean_reason\")\n",
    "    .agg(\n",
    "        pl.len().alias(\"num_compounds\"),\n",
    "        pl.col(\"canonical_smiles\").is_not_null().sum().alias(\"num_retained\"),\n",
    "    )\n",
    "    .sort(\"num_compounds\", descending=True)\n",
    ")\n",
    "\n",
    "reason_counts\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
